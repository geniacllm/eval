{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!ls -l /storage7/pretrain/Mixtral_3node_test/lr_1/epoch0-iter314999/convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"/storage7/pretrain/Mixtral_3node_test/lr_1/epoch0-iter314999/convert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizer(name_or_path='/storage7/pretrain/Mixtral_3node_test/lr_1/epoch0-iter314999/convert', vocab_size=56320, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 41/41 [00:07<00:00,  5.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MixtralForCausalLM(\n",
       "  (model): MixtralModel(\n",
       "    (embed_tokens): Embedding(56320, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-39): 40 x MixtralDecoderLayer(\n",
       "        (self_attn): MixtralSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): MixtralRotaryEmbedding()\n",
       "        )\n",
       "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
       "          (gate): Linear(in_features=2048, out_features=8, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-7): 8 x MixtralBlockSparseTop2MLP(\n",
       "              (w1): Linear(in_features=2048, out_features=4096, bias=False)\n",
       "              (w2): Linear(in_features=4096, out_features=2048, bias=False)\n",
       "              (w3): Linear(in_features=2048, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): MixtralRMSNorm()\n",
       "        (post_attention_layernorm): MixtralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MixtralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=56320, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,    32, 32132, 30152, 52207, 30154]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"今日の天気は\", return_tensors=\"pt\").to(model.device)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,    32, 32132, 30152, 52207, 30154, 38485,   292, 30643, 30170,\n",
       "         31736, 30156, 30531, 30339, 31962,   292, 51090, 30152, 31028,   291,\n",
       "         31736, 30156, 30531, 30221, 56215,   291, 32132, 30154, 30643, 30170,\n",
       "         31736, 30153, 30259, 30159, 31123, 30274, 30393, 30152, 33215, 30156,\n",
       "           291, 32844, 30237, 30170, 31736, 30156, 33701, 30189, 51246,   292,\n",
       "         55481,   291, 49931, 54965, 30583, 30826]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "outputs = model.generate(**inputs, max_new_tokens=50, repetition_penalty=1.1)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>今日の天気は晴れ。朝から雨が降っています。昨日の夜、雨が降ったので、今日は朝から雨になると思っていたのですが、昼前から雨が降り出しました。そして、夕方には止んで'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:  tensor([[    1,    32, 30215, 30157, 32591, 31008, 30265, 30154]])\n",
      "outputs:  tensor([    1,    32, 30215, 30157, 32591, 31008, 30265, 30154,   291, 32044,\n",
      "        30265, 30546, 31423, 33215,   292, 32044, 30265, 30152, 33063, 30153,\n",
      "        30163, 51590, 30281, 30152, 34488, 31227, 30513, 54965,   291, 32044,\n",
      "        30265, 30155, 37084, 30162, 31259, 31067, 33874, 30156, 51240,   292,\n",
      "        30193, 30203, 30166,   336, 32044, 30265, 51778, 30415,   337,   292,\n",
      "        32044, 30265, 30152, 33063, 30153, 30163, 56215,   291])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s>日本で一番高い山は、富士山だそうです。富士山の麓にある山梨県の山中湖村には、富士山を眺める絶景スポットがあります。その名も「富士山展望台」。富士山の麓にあるので、'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"日本で一番高い山は\", return_tensors=\"pt\", add_special_tokens=True).to(model.device)\n",
    "print(\"inputs: \", inputs.input_ids)\n",
    "outputs = model.generate(**inputs, max_new_tokens=50, repetition_penalty=1.1)\n",
    "print(\"outputs: \", outputs[0])\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
