{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JupyterLab on Slurm Cluster\n",
    "\n",
    "Install miniconda3, jupyterlab, and other packages. See [here](https://github.com/matsuolab/ucllm_nedo_prod/blob/main/infra/README.md#environment-preparation) and [here](https://www.notion.so/matsuolab-geniac/JupyterLab-96c369e0dc994dbfbb82f5c6385ba7bf?pvs=4) for setting up the environment.\n",
    "\n",
    "Create a batch file `jupyterlab.sh`.\n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --nodelist=slurm0-a3-ghpc-20\n",
    "#SBATCH --time=06:00:00\n",
    "#SBATCH --gpus-per-node=0\n",
    "#SBATCH --cpus-per-task=64\n",
    "#SBATCH --mem=64GB\n",
    "#SBATCH --job-name=jupyterlab\n",
    "#SBATCH --output=%x_%j.log\n",
    "\n",
    "source $EXP_HOME/miniconda3/etc/profile.d/conda.sh\n",
    "conda activate jupyter39\n",
    "\n",
    "jupyter-lab --no-browser --port 8888 --ip 0.0.0.0\n",
    "```\n",
    "\n",
    "Then, submit the job to the cluster.\n",
    "\n",
    "```\n",
    "sbatch jupyterlab.sh\n",
    "```\n",
    "\n",
    "Find the token in the log file.\n",
    "\n",
    "```\n",
    "tail -f jupyterlab_<job_id>.log\n",
    "```\n",
    "\n",
    "Setup the IAP tunneling and SSH port forwarding to access the JupyterLab on the cluster. See [here](https://www.notion.so/matsuolab-geniac/JupyterLab-96c369e0dc994dbfbb82f5c6385ba7bf?pvs=4#926966a09dd9474385f49d130127784b) for more details.\n",
    "\n",
    "Then open the browser and access the URL `http://localhost:8888/?token=<token>`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!ls -l /storage7/pretrain/Mixtral_3node_test/lr_1/epoch0-iter314999/convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"/storage7/pretrain/Mixtral_3node_test/lr_1/epoch0-iter314999/convert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizer(name_or_path='/storage7/pretrain/Mixtral_3node_test/lr_1/epoch0-iter314999/convert', vocab_size=56320, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 41/41 [00:09<00:00,  4.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MixtralForCausalLM(\n",
       "  (model): MixtralModel(\n",
       "    (embed_tokens): Embedding(56320, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-39): 40 x MixtralDecoderLayer(\n",
       "        (self_attn): MixtralSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): MixtralRotaryEmbedding()\n",
       "        )\n",
       "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
       "          (gate): Linear(in_features=2048, out_features=8, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-7): 8 x MixtralBlockSparseTop2MLP(\n",
       "              (w1): Linear(in_features=2048, out_features=4096, bias=False)\n",
       "              (w2): Linear(in_features=4096, out_features=2048, bias=False)\n",
       "              (w3): Linear(in_features=2048, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): MixtralRMSNorm()\n",
       "        (post_attention_layernorm): MixtralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MixtralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=56320, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,    32, 32132, 30152, 52207, 30154]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"今日の天気は\", return_tensors=\"pt\").to(model.device)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,    32, 30215, 30157, 32591, 31008, 30265, 30154,   291, 32044,\n",
       "         30265, 30546, 31423, 33215,   292, 32044, 30265, 30152, 48877, 30153,\n",
       "         30163, 30152, 30156,   291, 30774, 30243,   417,   290,   416,   423,\n",
       "           422,   512, 30152,   336, 30174, 38940, 32311,   337,   292, 32044,\n",
       "         30265, 30152, 48877, 30170, 30154,   291,   419,   422,   416, 30280,\n",
       "         30152, 41025, 33648, 30156, 35280, 31962,   292, 30232]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "outputs = model.generate(**inputs, max_new_tokens=50, repetition_penalty=1.1)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>今日の天気は晴れ。朝から雨が降っています。昨日の夜、雨が降ったので、今日は朝から雨になると思っていたのですが、昼前から雨が降り出しました。そして、夕方には止んで'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:  tensor([[    1,    32, 30215, 30157, 32591, 31008, 30265, 30154]])\n",
      "outputs:  tensor([    1,    32, 30215, 30157, 32591, 31008, 30265, 30154,   291, 32044,\n",
      "        30265, 30546, 31423, 33215,   292, 32044, 30265, 30152, 48877, 30153,\n",
      "        30163, 30152, 30156,   291, 30774, 30243,   417,   290,   416,   423,\n",
      "          422,   512, 30152,   336, 30174, 38940, 32311,   337,   292, 32044,\n",
      "        30265, 30152, 48877, 30170, 30154,   291,   419,   422,   416, 30280,\n",
      "        30152, 41025, 33648, 30156, 35280, 31962,   292, 30232])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s>日本で一番高い山は、富士山だそうです。富士山の頂上にあるのが、標高1,076mの「大汝峰」。富士山の頂上からは、360度のパノラマビューが楽しめます。また'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"日本で一番高い山は\", return_tensors=\"pt\", add_special_tokens=True).to(model.device)\n",
    "print(\"inputs: \", inputs.input_ids)\n",
    "outputs = model.generate(**inputs, max_new_tokens=50, repetition_penalty=1.1)\n",
    "print(\"outputs: \", outputs[0])\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
