{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install で 上の階層にあるlocalからインストールする see: https://stackoverflow.com/questions/15031694/installing-python-packages-from-local-file-system-folder-to-virtualenv-with-pip\n",
    "# %pip install -e ../../nano-askllm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache系は必ずteam storageへ\n",
    "# TEAM_DATASETS_CACHE_DIR=\"/persistentshare/storage/team_kumagai/datasets\"\n",
    "TEAM_DATASETS_CACHE_DIR = \"./.cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import wandb\n",
    "from huggingface_hub import login, whoami\n",
    "\n",
    "import argilla as rg\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    set_seed,\n",
    "    Seq2SeqTrainer,\n",
    "    BitsAndBytesConfig,\n",
    "    LlamaTokenizer,\n",
    "    TrainerCallback,\n",
    ")\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "from trl import DPOTrainer\n",
    "\n",
    "import torch.distributed as dist\n",
    "import multiprocessing as mp\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from typing import Any\n",
    "import torch\n",
    "\n",
    "logger = logging.getLogger()\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setLevel(logging.INFO)\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "logger.info(\"start logging...!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login(token=os.getenv(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    project=os.getenv(\"WANDB_PROJECT\"),\n",
    "    entity=os.getenv(\"WANDB_ENTITY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg.init(\n",
    "    api_url=os.getenv(\"RG_API_URL\"),\n",
    "    api_key=os.getenv(\"RG_API_KEY\"),\n",
    "    workspace=os.getenv(\"RG_WORKSPACE\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nano_askllm import AskLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Rakuten/RakutenAI-7B-instruct\"\n",
    "model2_id = \"Rakuten/RakutenAI-7B-chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=TEAM_DATASETS_CACHE_DIR)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, torch_dtype=\"auto\", device_map=\"auto\", cache_dir=TEAM_DATASETS_CACHE_DIR\n",
    ")\n",
    "model2 = AutoModelForCausalLM.from_pretrained(\n",
    "    model2_id, torch_dtype=\"auto\", device_map=\"auto\", cache_dir=TEAM_DATASETS_CACHE_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_prefix = \"###\\n\"\n",
    "prompt_template_postfix = \"\"\"\n",
    "###\n",
    "\n",
    "Does the previous paragraph demarcated within ### and ### contain informative signal for pre-training a large-language model? An informative datapoint should be well-formatted, contain some usable knowledge of the world, and strictly NOT have any harmful, racist, sexist, etc. content.\n",
    "\n",
    "OPTIONS: yes/no\n",
    "ANSWER:\"\"\"\n",
    "\n",
    "yes_tokens = [\"yes\", \"Yes\"]\n",
    "\n",
    "llm = AskLLM(\n",
    "    tokenizer,\n",
    "    model,\n",
    "    prompt_template_prefix=prompt_template_prefix,\n",
    "    prompt_template_postfix=prompt_template_postfix,\n",
    "    yes_tokens=yes_tokens,\n",
    "    max_tokens=512,  # You can increase it up to 8192 for Mistral-7B-v0.1 based models.\n",
    ")\n",
    "\n",
    "batch_size = 2\n",
    "num_ask = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoints = [\"sample\"]\n",
    "\n",
    "scores = llm.ask(datapoints)\n",
    "for score, datapoint in zip(scores.tolist(), datapoints):\n",
    "    text = datapoint[:40].replace(\"\\n\", \" \")\n",
    "    print(f\"score: {score:.4f}\\ttext: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ここから内部を取り出して同じことをみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ここで実装する\n",
    "datapoint = \"sample\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_tokens = tokenizer.encode(datapoint, add_special_tokens=True)\n",
    "encoded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated = datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_template_prefix + truncated + prompt_template_postfix\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([prompt], return_tensors=\"pt\", padding=True).to(model.device)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 1\n",
    "outputs = model.generate(\n",
    "    **inputs, max_new_tokens=max_new_tokens, output_logits=True, return_dict_in_generate=True\n",
    ")\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs.logits[0]\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "for i, prob in enumerate(probs):\n",
    "    tops = torch.topk(prob, k, dim=-1)\n",
    "    for j, (idx, val) in enumerate(zip(tops.indices, tops.values)):\n",
    "        print(f\"{tokenizer.decode(idx):8s}: {val.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yes_ids: torch.Tensor = (\n",
    "    tokenizer(yes_tokens, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    .to(model.device)\n",
    "    .input_ids[:, 0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yes_probs = probs[:, yes_ids]\n",
    "yes_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_len = sum(\n",
    "    [\n",
    "        len(tokenizer.encode(item, add_special_tokens=False))\n",
    "        for item in [prompt_template_prefix, prompt_template_postfix]\n",
    "    ]\n",
    ")\n",
    "prompt_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.sum(yes_probs, dim=-1)\n",
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
