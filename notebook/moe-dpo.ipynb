{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※以下の記載コマンドは本番環境向けではないので注意してください\n",
    "\n",
    "* megablocksによって作られたMoEモデルに対してtrl.DPOTrainerで事後学習ができるか確認する\n",
    "* 最低限のモデルができるところまで確認中（現在lossが下がっていない状態）\n",
    "\n",
    "```shell\n",
    "srun --job-name=kuma-eval --partition g2 --nodes=1 --gpus-per-node=8 --time=06:00:00 --mem=128GB --pty bash -i\n",
    "```\n",
    "\n",
    "で実行する。時間は使う時に調整する\n",
    "\n",
    "作業ノードでは以下を実行\n",
    "\n",
    "```shell\n",
    "conda activate venv39\n",
    "\n",
    "jupyter-lab --no-browser --port 8888 --ip $(hostname -i)\n",
    "```\n",
    "\n",
    "※VSCodeからnotebook作業する場合は[この対応](https://blog.masuyoshi.com/%E3%80%90vscode%E4%BD%BF%E7%94%A8%E8%80%85%E6%B3%A8%E6%84%8F%E3%80%91%E3%82%B5%E3%83%BC%E3%83%90%E3%83%BC%E3%83%AA%E3%82%BD%E3%83%BC%E3%82%B9%E9%A3%9F%E3%81%84%E6%95%A3%E3%82%89%E3%81%8B%E3%81%99/)をすること"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies は初回のみ実行\n",
    "%pip install ipywidgets bitsandbytes peft pyzmq transformers trl datasets sentencepiece accelerate wandb huggingface_hub argilla python-dotenv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache系は必ずteam storageへ\n",
    "# TEAM_DATASETS_CACHE_DIR=\"/persistentshare/storage/team_kumagai/datasets\"\n",
    "TEAM_DATASETS_CACHE_DIR=\"./.cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import wandb\n",
    "from huggingface_hub import login, whoami\n",
    "\n",
    "import argilla as rg\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    set_seed,\n",
    "    Seq2SeqTrainer,\n",
    "    BitsAndBytesConfig,\n",
    "    LlamaTokenizer,\n",
    "    TrainerCallback\n",
    ")\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "from trl import DPOTrainer\n",
    "\n",
    "import torch.distributed as dist\n",
    "import multiprocessing as mp\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "logger = logging.getLogger()\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setLevel(logging.INFO)\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "logger.info(\"start logging...!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login(token=os.getenv('HF_TOKEN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    project=os.getenv('WANDB_PROJECT'),\n",
    "    entity=os.getenv('WANDB_ENTITY'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg.init(\n",
    "    api_url=os.getenv(\"RG_API_URL\"),\n",
    "    api_key=os.getenv(\"RG_API_KEY\"),\n",
    "    workspace=os.getenv(\"RG_WORKSPACE\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def find_all_linear_names(peft_model, int4=False, int8=False):\n",
    "    \"\"\"Find all linear layer names in the model. reference from qlora paper.\"\"\"\n",
    "    cls = torch.nn.Linear\n",
    "    if int4 or int8:\n",
    "        import bitsandbytes as bnb\n",
    "        if int4:\n",
    "            cls = bnb.nn.Linear4bit\n",
    "        elif int8:\n",
    "            cls = bnb.nn.Linear8bitLt\n",
    "    lora_module_names = set()\n",
    "    for name, module in peft_model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            # last layer is not add to lora_module_names\n",
    "            if 'lm_head' in name:\n",
    "                continue\n",
    "            if 'output_layer' in name:\n",
    "                continue\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    return sorted(lora_module_names)\n",
    "\n",
    "\n",
    "def return_prompt_and_responses(examples) -> dict[str, str]:\n",
    "    \"\"\"Load the paired dataset and convert it to the necessary format.\n",
    "\n",
    "    The dataset is converted to a dictionary with the following structure:\n",
    "    {\n",
    "        'prompt': list[str],\n",
    "        'chosen': list[str],\n",
    "        'rejected': list[str],\n",
    "    }\n",
    "\n",
    "    Prompts are structured as follows:\n",
    "      \"Question: \" + <prompt> + \"\\n\\nAnswer: \"\n",
    "    \"\"\"\n",
    "    return {\n",
    "         # see: https://github.com/ZHZisZZ/emulated-disalignment/blob/2f8e441fdf9117490c36d9f54adf536c23b6eb69/utils/utils.py#L80\n",
    "        \"prompt\": [\"Question: \" + question.split(\"\\n\\nAssistant\")[0].split(\"\\n\\nHuman: \")[1] + \"\\n\\nAnswer: \" for question in examples[\"chosen\"]],\n",
    "        \"chosen\": examples[\"chosen\"],\n",
    "        \"rejected\": examples[\"rejected\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"geniacllm/dMoEHf2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(model_name,\n",
    "                                    cache_dir=TEAM_DATASETS_CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install MoE model created by megablocks\n",
    "# prepare model\n",
    "model_moe = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=TEAM_DATASETS_CACHE_DIR,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_trainable_parameters(model_moe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install tokenizer\n",
    "# tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    # use_fast=False,\n",
    "    # add_eos_token=True,\n",
    "    # trust_remote_code=True,\n",
    "    cache_dir=TEAM_DATASETS_CACHE_DIR,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "# なんかおかしい？ので足してみる\n",
    "# 出たエラー: ValueError: Padding is enabled, but the tokenizer is not configured with a padding token. Explicitly set `tokenizer.pad_token` (e.g. `tokenizer.pad_token = tokenizer.eos_token`) before calling the trainer.\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "eos_token_text = tokenizer.eos_token\n",
    "print(f'eos token: {eos_token_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install datasets for DPO\n",
    "dataset_rlhf = load_dataset(\"Anthropic/hh-rlhf\", cache_dir=TEAM_DATASETS_CACHE_DIR)\n",
    "dataset_rlhf_ja = load_dataset(\"llm-jp/hh-rlhf-12k-ja\", cache_dir=TEAM_DATASETS_CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_rlhf)\n",
    "print(dataset_rlhf_ja)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "megablocksで作られたモデルでHFに上がったものを使って、DPOを試す\n",
    "\n",
    "see: https://llama2-accessory.readthedocs.io/en/latest/projects/mixtral-8x7b.html\n",
    "\n",
    "ref: https://github.com/shibing624/MedicalGPT/blob/726bd2a62686bd7ed62262be44f8e0233edc2443/dpo_training.py#L30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset for train\n",
    "max_source_length = 256\n",
    "max_target_length = 256\n",
    "full_max_length = max_source_length + max_target_length\n",
    "\n",
    "raw_datasets = dataset_rlhf\n",
    "\n",
    "if \"train\" not in raw_datasets:\n",
    "    raise ValueError(\"--do_train requires a train dataset\")\n",
    "train_dataset = raw_datasets['train']\n",
    "max_train_samples = len(train_dataset)\n",
    "logger.debug(f\"Example train_dataset[0]: {train_dataset[0]}\")\n",
    "\n",
    "tokenized_dataset = train_dataset.shuffle().map(\n",
    "    return_prompt_and_responses,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "train_dataset = tokenized_dataset.filter(\n",
    "    lambda x: 0 < len(x['prompt'] + x['chosen']) <= full_max_length\n",
    "                and 0 < len(x['prompt'] + x['rejected']) <= full_max_length\n",
    ")\n",
    "logger.debug(f\"Num train_samples: {len(train_dataset)}\")\n",
    "logger.debug(\"First train example:\")\n",
    "logger.debug(train_dataset[0]['prompt'] + train_dataset[0]['chosen'])\n",
    "\n",
    "if \"test\" not in raw_datasets:\n",
    "    raise ValueError(\"--do_eval requires a test dataset\")\n",
    "eval_dataset = raw_datasets[\"test\"]\n",
    "max_eval_samples = len(eval_dataset)\n",
    "logger.debug(f\"Example eval_dataset[0]: {eval_dataset[0]}\")\n",
    "\n",
    "eval_dataset = eval_dataset.map(\n",
    "    return_prompt_and_responses,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=eval_dataset.column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "eval_dataset = eval_dataset.filter(\n",
    "    lambda x: 0 < len(x['prompt'] + x['chosen']) <= full_max_length\n",
    "                and 0 < len(x['prompt'] + x['rejected']) <= full_max_length\n",
    ")\n",
    "logger.debug(f\"Num eval_samples: {len(eval_dataset)}\")\n",
    "logger.debug(\"First eval example:\")\n",
    "logger.debug(eval_dataset[0]['prompt'] + eval_dataset[0]['chosen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see: https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments\n",
    "# github: https://github.com/huggingface/transformers/blob/v4.39.3/src/transformers/training_args.py#L176\n",
    "training_args = TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        max_steps=50,\n",
    "        logging_steps=1,\n",
    "        save_steps=5,\n",
    "        gradient_accumulation_steps=1,\n",
    "        gradient_checkpointing=False,\n",
    "        learning_rate=5e-4,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=5,\n",
    "        output_dir=\"./output-dpo\",\n",
    "        report_to=[\"wandb\"],\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_steps=2,\n",
    "        optim=\"paged_adamw_32bit\", # see: https://github.com/eyess-glitch/phi-2-fine-tuning/blob/79cd01554482973f5e709ca9da9a5746d305b46e/dpo_train.py#L34\n",
    "        bf16=True,  # T4はbf16が使えないけどL4は使える\n",
    "        fp16=False,\n",
    "        remove_unused_columns=False,\n",
    "        run_name=f\"dpo_{config.model_type}\",\n",
    "        # device_map=\"auto\", の指定はない\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer\n",
    "trainer_dpo = DPOTrainer(\n",
    "    model_moe,\n",
    "    ref_model=None,\n",
    "    args=training_args,\n",
    "    beta=0.1,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=None,\n",
    "    max_prompt_length=1024,\n",
    "    max_length=full_max_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start!\n",
    "trainer_dpo.train()\n",
    "trainer_dpo.model.save_pretrained(\"./model-dpo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push HF see: https://github.com/huggingface/transformers/blob/v4.27.2/src/transformers/trainer.py#L3559\n",
    "\n",
    "# trainer_dpo.push_to_hub(\"geniacllm/dMoEHf2-dpo-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
